{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 579 - ASSIGNMENT 3\n",
    "[Ling Fei Zhang](https://github.com/Ling01234), 260985358\n",
    "\n",
    "Sevag Baghdassarian, ID\n",
    "\n",
    "Brandon Ma, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import time\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import sem\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward after simulate_episode: [96, 141, 128, 95, 132, 146, 107, 111, 109, 98, 107, 86, 112, 142, 132, 91, 118, 144, 131, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 317.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model reward is [45.0, 153.0, 69.0, 155.0, 51.0, 138.0, 181.0, 60.0, 141.0, 61.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2042.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random agent reward is [9.0, 11.0, 8.0, 9.0, 10.0, 9.0, 10.0, 9.0, 10.0, 8.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Actions:\n",
    "# 0: left\n",
    "# 1: right\n",
    "\n",
    "# best params initialization:\n",
    "ALPHA = 1/4\n",
    "EPSILON = 0.25\n",
    "GAMMA = 0.95\n",
    "BINS = 10\n",
    "EPISODES = 1000\n",
    "RUNS = 10\n",
    "\n",
    "\n",
    "class Qlearning:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, num_episodes, num_bins, seed) -> None:\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_bins = num_bins\n",
    "        self.lowerbound = env.observation_space.low\n",
    "        self.lowerbound[1] = -3.5\n",
    "        self.lowerbound[3] = -10\n",
    "        self.upperbound = env.observation_space.high\n",
    "        self.upperbound[1] = 3.5\n",
    "        self.upperbound[3] = 10\n",
    "        # self.env.seed(seed)\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.num_action = env.action_space.n\n",
    "        self.reward = []\n",
    "        self.Qvalues = np.random.uniform(low=-0.001, high=0.001,\n",
    "                                         size=(num_bins, num_bins, num_bins, num_bins, self.num_action))\n",
    "        self.behavior_episodes = []\n",
    "        self.random_episodes = []\n",
    "        self.bins = []\n",
    "        for i in range(4):\n",
    "            self.bins.append(np.linspace(\n",
    "                self.lowerbound[i], self.upperbound[i], self.num_bins))\n",
    "            \n",
    "    # def get_behavior_episodes(self):\n",
    "    #     return np.apply_along_axis(np.argmax, 4, self.Qvalues) #shape (10, 10, 10, 10)\n",
    "        \n",
    "\n",
    "    def discritize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discritize continuous state into a discrete state\n",
    "\n",
    "        Args:\n",
    "            state (list of length 4): Current continuous state of agent\n",
    "\n",
    "        Returns:\n",
    "            state (4-tuple): Current discritized state of agent\n",
    "        \"\"\"\n",
    "        new_state = []\n",
    "        for i in range(4):\n",
    "            index = np.maximum(np.digitize(state[i], self.bins[i]) - 1, 0)\n",
    "            new_state.append(index)\n",
    "\n",
    "        return tuple(new_state)\n",
    "\n",
    "    def select_action(self, state, episode):\n",
    "        \"\"\"\n",
    "        Select action given a state\n",
    "\n",
    "        Args:\n",
    "            state (4-tuple): Current state of the agent, continuous\n",
    "            episode (int): Current episode of the run\n",
    "\n",
    "        Returns:\n",
    "            int: Action chosen by the agent\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # lower exploration rate as we run many episodes\n",
    "        if episode > 700:\n",
    "            self.epsilon *= 0.99\n",
    "\n",
    "        # epsilon greedy\n",
    "        number = np.random.random()\n",
    "        if number < self.epsilon:  # uniformly choose action\n",
    "            return np.random.choice(self.num_action)\n",
    "\n",
    "        # greedy selection\n",
    "        state = self.discritize_state(state)\n",
    "        best_actions = np.where(\n",
    "            self.Qvalues[state] == np.max(self.Qvalues[state]))[0]\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "    def simulate_episodes(self):\n",
    "        \"\"\"\n",
    "        Simulate a specified number of episodes\n",
    "        \"\"\"\n",
    "        for episode in range(1, self.num_episodes+1):\n",
    "            # reset env\n",
    "            (state, _) = self.env.reset()\n",
    "            state = list(state)\n",
    "\n",
    "            # run episode\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                action = self.select_action(state, episode)\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                next_discritized_state = self.discritize_state(\n",
    "                    list(next_state))\n",
    "\n",
    "                q_max = np.max(self.Qvalues[next_discritized_state])\n",
    "                self.qlearning_update(\n",
    "                    terminal, reward, action, discritized_state, q_max)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            self.reward.append(int(episode_reward))\n",
    "\n",
    "    def qlearning_update(self, terminal, reward, action, state, q_max):\n",
    "        \"\"\"\n",
    "        Qlearning update rule\n",
    "\n",
    "        Args:\n",
    "            terminal (bool): True if at terminal state, False otherwise\n",
    "            reward (int): Reward of the agent at current state\n",
    "            action (int): Action taken by agent\n",
    "            state (4-tuple): Discrete state of the agent\n",
    "            q_max (float): Max Q value of the next state\n",
    "        \"\"\"\n",
    "        if not terminal:\n",
    "            loss = reward + self.gamma * q_max - \\\n",
    "                self.Qvalues[state + (action,)]\n",
    "        else:\n",
    "            loss = reward - self.Qvalues[state + (action,)]\n",
    "\n",
    "        self.Qvalues[state + (action,)] += self.alpha * loss\n",
    "\n",
    "    def visualize(self, games):\n",
    "        \"\"\"\n",
    "        Visualize the game played for a specified number of games.\n",
    "        Prints out the reward for each game.\n",
    "\n",
    "        Args:\n",
    "            games (int): Number of games to be played\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        for game in range(games):\n",
    "            (state, _) = env.reset()\n",
    "            env.render()\n",
    "            rewards = 0\n",
    "\n",
    "            for _ in range(500):\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                best_actions = np.where(self.Qvalues[discritized_state] == np.max(\n",
    "                    self.Qvalues[discritized_state]))[0]\n",
    "                action = np.random.choice(best_actions)\n",
    "                (state, reward, terminal, _, _) = env.step(action)\n",
    "                rewards += int(reward)\n",
    "                time.sleep(0.05)\n",
    "\n",
    "                if terminal:\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "            print(f\"reward for game {game}: {rewards}\")\n",
    "        env.close()\n",
    "        \n",
    "    def gather_episodes_agent(self):\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        self.simulate_episodes()\n",
    "        print(f\"reward after simulate_episode: {self.reward[-20:]}\")\n",
    "        for episode in trange(1, 501):\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"gather episode {episode}\")\n",
    "            state, _ = env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                best_actions = np.where(self.Qvalues[discritized_state] == np.max(\n",
    "                    self.Qvalues[discritized_state]))[0]\n",
    "                action = np.random.choice(best_actions)\n",
    "                (next_state, reward, terminal, _, _) = env.step(action)\n",
    "                self.behavior_episodes.append((state, action))\n",
    "                state = next_state\n",
    "        \n",
    "        \n",
    "        # preprocess data\n",
    "        x = np.array([data[0] for data in self.behavior_episodes])\n",
    "        y = np.array([data[1] for data in self.behavior_episodes])\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(x, y)\n",
    "        \n",
    "        # test model for 10 games \n",
    "        model_reward = []\n",
    "        for game in range(10):\n",
    "            # print(f\"game {game}\")\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                action = model.predict(state.reshape(1, -1))[0]\n",
    "                # print(action)\n",
    "                state, reward, terminal, *_ = env.step(action)\n",
    "                episode_reward += reward\n",
    "            \n",
    "            model_reward.append(episode_reward)\n",
    "        \n",
    "        env.close()        \n",
    "        print(f\"The model reward is {model_reward}\")\n",
    "        return model_reward\n",
    "    \n",
    "    def gather_episodes_random(self):\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        # print(f\"reward after simulate_episode: {self.reward[-20:]}\")\n",
    "        for episode in trange(1, 501):\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"random agent episode {episode}\")\n",
    "            state, _ = env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                action = env.action_space.sample()\n",
    "                (next_state, reward, terminal, _, _) = env.step(action)\n",
    "                self.random_episodes.append((state, action))\n",
    "                state = next_state\n",
    "        \n",
    "        \n",
    "        # preprocess data\n",
    "        x = np.array([data[0] for data in self.random_episodes])\n",
    "        y = np.array([data[1] for data in self.random_episodes])\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(x, y)\n",
    "        \n",
    "        # test model for 10 games \n",
    "        model_reward = []\n",
    "        for game in range(10):\n",
    "            # print(f\"game {game}\")\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                action = model.predict(state.reshape(1, -1))[0]\n",
    "                # print(action)\n",
    "                state, reward, terminal, *_ = env.step(action)\n",
    "                episode_reward += reward\n",
    "            \n",
    "            model_reward.append(episode_reward)\n",
    "        \n",
    "        env.close()        \n",
    "        print(f\"The random agent reward is {model_reward}\")\n",
    "        return model_reward\n",
    "        \n",
    "        \n",
    "\n",
    "def test_model():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    qlearning = Qlearning(env, ALPHA, GAMMA, EPSILON, EPISODES, BINS, 123)\n",
    "    model_rewards = qlearning.gather_episodes_agent()\n",
    "    random_rewards = qlearning.gather_episodes_random()\n",
    "    env.close()\n",
    "    \n",
    "test_model()    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have pre-trained our Q-Learning agent with 1000 episodes, as we did in the previous assignment. We then use the trained Q-Learning agent as our expert in simple imitation learning and used logisitc regression to imitate the action observed in each state. We can see that we can get decent results from simple imitation learning, as we score in the mid hundreds about half the time throughout a 10 games testing. In fact, we observe that some of the rewards returned by simple imitation is even higher than the latest 20 returns by our Q-Learning agent (expert) itself. However, simple imitation learning is not really consistent, as it has returns less than 100 about half the time. On the other hand, we can see the returns received by the random agent. Without surprise, the returns are very low. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
