{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 579 - Assignment 3\n",
    "[Ling Fei Zhang](https://github.com/Ling01234), 260985358\n",
    "\n",
    "Sevag Baghdassarian, ID\n",
    "\n",
    "Brandon Ma, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import time\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import sem\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions:\n",
    "# 0: left\n",
    "# 1: right\n",
    "\n",
    "# best params initialization:\n",
    "ALPHA = 1/4\n",
    "EPSILON = 0.25\n",
    "GAMMA = 0.95\n",
    "BINS = 10\n",
    "EPISODES = 1000\n",
    "RUNS = 10\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "class Qlearning:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, num_episodes, num_bins, seed) -> None:\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_bins = num_bins\n",
    "        self.lowerbound = env.observation_space.low\n",
    "        self.lowerbound[1] = -3.5\n",
    "        self.lowerbound[3] = -10\n",
    "        self.upperbound = env.observation_space.high\n",
    "        self.upperbound[1] = 3.5\n",
    "        self.upperbound[3] = 10\n",
    "        # self.env.seed(seed)\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.num_action = env.action_space.n\n",
    "        self.reward = []\n",
    "        self.Qvalues = np.random.uniform(low=-0.001, high=0.001,\n",
    "                                         size=(num_bins, num_bins, num_bins, num_bins, self.num_action))\n",
    "        self.behavior_episodes1 = []\n",
    "        self.behavior_episodes2 = []\n",
    "        self.behavior_episodes3 = []\n",
    "        self.random_episodes1 = []\n",
    "        self.random_episodes2 = []\n",
    "        self.random_episodes3 = []\n",
    "        self.bins = []\n",
    "        for i in range(4):\n",
    "            self.bins.append(np.linspace(\n",
    "                self.lowerbound[i], self.upperbound[i], self.num_bins))\n",
    "            \n",
    "    # def get_behavior_episodes(self):\n",
    "    #     return np.apply_along_axis(np.argmax, 4, self.Qvalues) #shape (10, 10, 10, 10)\n",
    "        \n",
    "\n",
    "    def discritize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discritize continuous state into a discrete state\n",
    "\n",
    "        Args:\n",
    "            state (list of length 4): Current continuous state of agent\n",
    "\n",
    "        Returns:\n",
    "            state (4-tuple): Current discritized state of agent\n",
    "        \"\"\"\n",
    "        new_state = []\n",
    "        for i in range(4):\n",
    "            index = np.maximum(np.digitize(state[i], self.bins[i]) - 1, 0)\n",
    "            new_state.append(index)\n",
    "\n",
    "        return tuple(new_state)\n",
    "\n",
    "    def select_action(self, state, episode):\n",
    "        \"\"\"\n",
    "        Select action given a state\n",
    "\n",
    "        Args:\n",
    "            state (4-tuple): Current state of the agent, continuous\n",
    "            episode (int): Current episode of the run\n",
    "\n",
    "        Returns:\n",
    "            int: Action chosen by the agent\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # lower exploration rate as we run many episodes\n",
    "        if episode > 700:\n",
    "            self.epsilon *= 0.99\n",
    "\n",
    "        # epsilon greedy\n",
    "        number = np.random.random()\n",
    "        if number < self.epsilon:  # uniformly choose action\n",
    "            return np.random.choice(self.num_action)\n",
    "\n",
    "        # greedy selection\n",
    "        state = self.discritize_state(state)\n",
    "        best_actions = np.where(\n",
    "            self.Qvalues[state] == np.max(self.Qvalues[state]))[0]\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "    def simulate_episodes(self):\n",
    "        \"\"\"\n",
    "        Simulate a specified number of episodes\n",
    "        \"\"\"\n",
    "        for episode in range(1, self.num_episodes+1):\n",
    "            # reset env\n",
    "            (state, _) = self.env.reset()\n",
    "            state = list(state)\n",
    "\n",
    "            # run episode\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                action = self.select_action(state, episode)\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                next_discritized_state = self.discritize_state(\n",
    "                    list(next_state))\n",
    "\n",
    "                q_max = np.max(self.Qvalues[next_discritized_state])\n",
    "                self.qlearning_update(\n",
    "                    terminal, reward, action, discritized_state, q_max)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            self.reward.append(int(episode_reward))\n",
    "\n",
    "    def qlearning_update(self, terminal, reward, action, state, q_max):\n",
    "        \"\"\"\n",
    "        Qlearning update rule\n",
    "\n",
    "        Args:\n",
    "            terminal (bool): True if at terminal state, False otherwise\n",
    "            reward (int): Reward of the agent at current state\n",
    "            action (int): Action taken by agent\n",
    "            state (4-tuple): Discrete state of the agent\n",
    "            q_max (float): Max Q value of the next state\n",
    "        \"\"\"\n",
    "        if not terminal:\n",
    "            loss = reward + self.gamma * q_max - \\\n",
    "                self.Qvalues[state + (action,)]\n",
    "        else:\n",
    "            loss = reward - self.Qvalues[state + (action,)]\n",
    "\n",
    "        self.Qvalues[state + (action,)] += self.alpha * loss\n",
    "\n",
    "    def visualize(self, games):\n",
    "        \"\"\"\n",
    "        Visualize the game played for a specified number of games.\n",
    "        Prints out the reward for each game.\n",
    "\n",
    "        Args:\n",
    "            games (int): Number of games to be played\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        for game in range(games):\n",
    "            (state, _) = env.reset()\n",
    "            env.render()\n",
    "            rewards = 0\n",
    "\n",
    "            for _ in range(500):\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                best_actions = np.where(self.Qvalues[discritized_state] == np.max(\n",
    "                    self.Qvalues[discritized_state]))[0]\n",
    "                action = np.random.choice(best_actions)\n",
    "                (state, reward, terminal, _, _) = env.step(action)\n",
    "                rewards += int(reward)\n",
    "                time.sleep(0.05)\n",
    "\n",
    "                if terminal:\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "            print(f\"reward for game {game}: {rewards}\")\n",
    "        env.close()\n",
    "        \n",
    "    def gather_episodes_agent(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Gather num_episodes behavior episodes for simple imitation learning\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): number of behavior episodes desired\n",
    "\n",
    "        Returns:\n",
    "            int: return of simple imitation learning using Q-Learning Agent\n",
    "            as expert.\n",
    "        \"\"\"\n",
    "        self.simulate_episodes()\n",
    "        print(f\"reward after simulate_episode: {self.reward[-20:]}\")\n",
    "        for episode in trange(1, num_episodes+1):\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"gather episode {episode}\")\n",
    "            state, _ = self.env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                best_actions = np.where(self.Qvalues[discritized_state] == np.max(\n",
    "                    self.Qvalues[discritized_state]))[0]\n",
    "                action = np.random.choice(best_actions)\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                discritized_next_state = self.discritize_state(next_state)\n",
    "                \n",
    "                if episode <= 100:\n",
    "                    self.behavior_episodes1.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                    self.behavior_episodes2.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                if episode <= 250:\n",
    "                    self.behavior_episodes2.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                self.behavior_episodes3.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "    \n",
    "    def gather_episodes_random(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Gather num_episodes behavior episodes with a random agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): number of behavior episodes desired for simple imitation\n",
    "\n",
    "        Returns:\n",
    "            int: return by a random agent. \n",
    "        \"\"\"\n",
    "        # print(f\"reward after simulate_episode: {self.reward[-20:]}\")\n",
    "        for episode in trange(1, num_episodes+1):\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"random agent episode {episode}\")\n",
    "            state, _ = self.env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                action = self.env.action_space.sample()\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                discritized_next_state = self.discritize_state(next_state)\n",
    "                \n",
    "                if episode <= 100:\n",
    "                    self.random_episodes1.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                    self.random_episodes2.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                if episode <= 250:\n",
    "                    self.random_episodes2.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                self.random_episodes3.append((discritized_state, action, reward, discritized_next_state, terminal))\n",
    "                \n",
    "                state = next_state\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleImitation:\n",
    "    def __init__(self, env, dataset) -> None:\n",
    "        self.env = env\n",
    "        self.dataset = dataset\n",
    "        self.lowerbound = env.observation_space.low\n",
    "        self.lowerbound[1] = -3.5\n",
    "        self.lowerbound[3] = -10\n",
    "        self.upperbound = env.observation_space.high\n",
    "        self.upperbound[1] = 3.5\n",
    "        self.upperbound[3] = 10\n",
    "        self.bins = []\n",
    "        for i in range(4):\n",
    "            self.bins.append(np.linspace(\n",
    "                self.lowerbound[i], self.upperbound[i], 10))\n",
    "            \n",
    "            \n",
    "    def discritize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discritize continuous state into a discrete state\n",
    "\n",
    "        Args:\n",
    "            state (list of length 4): Current continuous state of agent\n",
    "\n",
    "        Returns:\n",
    "            state (4-tuple): Current discritized state of agent\n",
    "        \"\"\"\n",
    "        new_state = []\n",
    "        for i in range(4):\n",
    "            index = np.maximum(np.digitize(state[i], self.bins[i]) - 1, 0)\n",
    "            new_state.append(index)\n",
    "\n",
    "        return tuple(new_state)\n",
    "\n",
    "\n",
    "    def forward(self, num_episode = 1):\n",
    "        model = LogisticRegression()\n",
    "        x = np.array([data[0] for data in self.dataset])\n",
    "        y = np.array([data[1] for data in self.dataset])\n",
    "        model.fit(x, y)\n",
    "        \n",
    "        rewards = []\n",
    "        for _ in range(1, num_episode + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                state = np.array(self.discritize_state(state))\n",
    "                action = model.predict(state.reshape(1, -1))[0]\n",
    "                state, reward, terminal, *_ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "            rewards.append(int(episode_reward))\n",
    "        return rewards\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will run a test run on the model, and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward after simulate_episode: [75, 106, 99, 78, 92, 111, 93, 68, 91, 98, 104, 105, 97, 117, 114, 112, 98, 97, 106, 99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 227.60it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1232.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple imitation with expert agent reward: [105]\n",
      "Simple imitation with random agent reward: [11]\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    qlearning = Qlearning(env, ALPHA, GAMMA, EPSILON, EPISODES, BINS, SEED)\n",
    "    qlearning.gather_episodes_agent(500)\n",
    "    qlearning.gather_episodes_random(500)\n",
    "    imitation_expert = SimpleImitation(env, qlearning.behavior_episodes3)\n",
    "    expert_reward = imitation_expert.forward()\n",
    "    imitation_random = SimpleImitation(env, qlearning.random_episodes3)\n",
    "    random_reward = imitation_random.forward()\n",
    "    print(f\"Simple imitation with expert agent reward: {expert_reward}\")\n",
    "    print(f\"Simple imitation with random agent reward: {random_reward}\")\n",
    "    env.close()\n",
    "    return qlearning.behavior_episodes1, qlearning.random_episodes1, qlearning.behavior_episodes2, qlearning.random_episodes2, qlearning.behavior_episodes3, qlearning.random_episodes3\n",
    "    \n",
    "data_expert100, data_random100, data_expert250, data_random250, data_expert500, data_random500 = test_model()    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-trained our Q-Learning agent with 1000 episodes, as we did in the previous assignment. We then use the trained Q-Learning agent as our expert in simple imitation learning and used logisitc regression to imitate the action observed in each state. The results above were produced using 500 behavior episodes. We can see that we can get decent results from simple imitation learning. On the other hand, we can see the returns received by the random agent. Without surprise, the returns are very low. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Below, we will create the mixed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffled data\n",
    "mixed_data_expert = random.sample(data_expert500, 500)\n",
    "mixed_data_random = random.sample(data_random500, 500)\n",
    "mixed_data = mixed_data_expert + mixed_data_random\n",
    "\n",
    "# Mixed data\n",
    "mixed_data100 = mixed_data_expert[:int(len(mixed_data)/10)]\n",
    "mixed_data250 = mixed_data_expert[:int(len(mixed_data)/4)]\n",
    "mixed_data500 = mixed_data_expert[:int(len(mixed_data)/2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitted Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FittedQLearning:\n",
    "    def __init__(self, env, buffer, gamma=0.99, num_episodes=500, batch_size=64, buffer_size=5000, approximator=\"linear\"):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = buffer\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.approximator = approximator\n",
    "        self.Qvalues = np.random.uniform(low=-0.001, high=0.001,\n",
    "                                         size=(10, 10, 10, 10, self.action_space))\n",
    "        self.lowerbound = env.observation_space.low\n",
    "        self.lowerbound[1] = -3.5\n",
    "        self.lowerbound[3] = -10\n",
    "        self.upperbound = env.observation_space.high\n",
    "        self.upperbound[1] = 3.5\n",
    "        self.upperbound[3] = 10\n",
    "        self.bins = []\n",
    "        for i in range(4):\n",
    "            self.bins.append(np.linspace(\n",
    "                self.lowerbound[i], self.upperbound[i], 10))\n",
    "        self.reward = []\n",
    "        \n",
    "    \n",
    "    def discritize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discritize continuous state into a discrete state\n",
    "\n",
    "        Args:\n",
    "            state (list of length 4): Current continuous state of agent\n",
    "\n",
    "        Returns:\n",
    "            state (4-tuple): Current discritized state of agent\n",
    "        \"\"\"\n",
    "        new_state = []\n",
    "        for i in range(4):\n",
    "            index = np.maximum(np.digitize(state[i], self.bins[i]) - 1, 0)\n",
    "            new_state.append(index)\n",
    "\n",
    "        return tuple(new_state)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Fitted Q Learning update step. This is done in batch.\n",
    "        For linear function approximation, we are using tabular RL.\n",
    "        We can directly minimize the mean square error to 0 by replacing\n",
    "        the qvalue of a (state, action) pair by the target qvalue. \n",
    "        \"\"\"\n",
    "        if len(self.buffer) > self.batch_size:\n",
    "            batch_index = np.random.choice(len(self.buffer), size=self.batch_size)\n",
    "            batch = [self.buffer[i] for i in batch_index]\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = np.array(states)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            next_states = np.array(next_states)\n",
    "            dones = np.array(dones)\n",
    "            \n",
    "            max_qvalues = np.zeros(self.batch_size)\n",
    "            for index, next_state, in enumerate(next_states):\n",
    "                if not dones[index]:\n",
    "                    next_state = self.discritize_state(list(next_state))\n",
    "                    max_qvalues[index] = np.max(self.Qvalues[next_state]) \n",
    "            \n",
    "            targets = rewards + self.gamma * max_qvalues # 1 x batch_size\n",
    "            \n",
    "            for i, target in enumerate(targets):\n",
    "                state = states[i]\n",
    "                state = tuple(state)\n",
    "                action = actions[i]\n",
    "                self.Qvalues[state + (action,)] = target\n",
    "                \n",
    "                \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action based on current state\n",
    "\n",
    "        Args:\n",
    "            state (4-tuple): current state of agent, discritized\n",
    "\n",
    "        Returns:\n",
    "            int: action taken based on greedy selection (i.e. no exploration)\n",
    "        \"\"\"\n",
    "        best_actions = np.where(\n",
    "            self.Qvalues[state] == np.max(self.Qvalues[state]))[0]\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        add a sample to the replay buffer\n",
    "\n",
    "        Args:\n",
    "            state (4-tuple): current state of the agent\n",
    "            action (int): action taken by agent\n",
    "            reward (int): reward received\n",
    "            next_state (4-tuple): next state of agent\n",
    "            done (bool): whether the next step terminates the game\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent on a specified number of episodes\n",
    "        \"\"\"\n",
    "        for _ in trange(1, self.num_episodes+1):\n",
    "            self.update()\n",
    "\n",
    "    def test(self, num_episode=1):\n",
    "        \"\"\"\n",
    "        Test the trained fitted q learning agent on a simple game\n",
    "        and record the return of the agent.\n",
    "\n",
    "        Returns:\n",
    "            int: return of the agent\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        for episode in range(1, num_episode + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                state = self.discritize_state(list(state))\n",
    "                action = self.select_action(state)\n",
    "                state, reward, done, *_ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            rewards.append(int(episode_reward))\n",
    "        return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 792.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Q Learning with expert data reward: [99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 835.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Q Learning with random data reward: [84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 812.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Q Learning with mixed data reward: [105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_model_fitted():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    model = FittedQLearning(env, data_expert500)\n",
    "    model.train()\n",
    "    reward = model.test()\n",
    "    print(f\"Fitted Q Learning with expert data reward: {reward}\")\n",
    "    model = FittedQLearning(env, data_random500)\n",
    "    model.train()\n",
    "    reward = model.test()\n",
    "    print(f\"Fitted Q Learning with random data reward: {reward}\")\n",
    "    model = FittedQLearning(env, mixed_data500)\n",
    "    model.train()\n",
    "    reward = model.test()\n",
    "    print(f\"Fitted Q Learning with mixed data reward: {reward}\")\n",
    "    env.close()\n",
    "    \n",
    "test_model_fitted()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can clearly see the effect of the datasets. With the expert data, we have the best return by agent. This is to be expected, as the agent used the best data there is. Then, we observed a very low return by the agent when trained using random data. This is essentially the same as not learned anything, i.e. just taking actions randomly. Lastly, when we mixed the expert data and the random data, we observed a decent return, but not as good as the return by the first agent. This is also natural, as the agent was able to partly learn from the expert data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function to plot the bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(size):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    if size == 100:\n",
    "        expert = data_expert100\n",
    "        rand = data_random100\n",
    "        mixed = mixed_data100\n",
    "    elif size == 250:\n",
    "        expert = data_expert250\n",
    "        rand = data_random250\n",
    "        mixed = mixed_data250\n",
    "    else: # size == 500\n",
    "        expert = data_expert500\n",
    "        rand = data_random500\n",
    "        mixed = mixed_data500\n",
    "        \n",
    "    imitation_expert = SimpleImitation(env, expert)\n",
    "    imitation_reward_expert = imitation_expert.forward(100)\n",
    "    imitation_reward_expert = np.mean(imitation_reward_expert)\n",
    "    \n",
    "    imitation_random = SimpleImitation(env, rand)\n",
    "    imitation_reward_random = imitation_random.forward(100)\n",
    "    imitation_reward_random = np.mean(imitation_reward_random)\n",
    "    \n",
    "    imitation_mixed = SimpleImitation(env, mixed)\n",
    "    imitation_reward_mixed = imitation_mixed.forward(100)\n",
    "    imitation_reward_mixed = np.mean(imitation_reward_mixed)\n",
    "    \n",
    "    fitted_expert = FittedQLearning(env, expert)\n",
    "    fitted_expert.train()\n",
    "    fitted_reward_expert = fitted_expert.test(100)\n",
    "    fitted_reward_expert = np.mean(fitted_reward_expert)\n",
    "    \n",
    "    fitted_random = FittedQLearning(env, rand)\n",
    "    fitted_random.train()\n",
    "    fitted_reward_random= fitted_random.test(100)\n",
    "    fitted_reward_random= np.mean(fitted_reward_random)\n",
    "    \n",
    "    fitted_mixed = FittedQLearning(env, mixed)\n",
    "    fitted_mixed.train()\n",
    "    fitted_reward_mixed = fitted_mixed.test(100)\n",
    "    fitted_reward_mixed = np.mean(fitted_reward_mixed)\n",
    "    \n",
    "    model = [\"Simple Imitation Expert\", \"Simple Imitation Random\", \"Simple Imitation Mixed\", \"FittedQLearning Expert\", \"FittedQLearning Random\", \"FittedQLearning Mixed\"]\n",
    "    returns = [imitation_reward_expert, imitation_reward_random, imitation_reward_mixed, fitted_reward_expert, fitted_reward_random, fitted_reward_mixed]\n",
    "    colors = [mcolors.TABLEAU_COLORS[\"tab:blue\"],\n",
    "              mcolors.TABLEAU_COLORS[\"tab:green\"], \n",
    "              mcolors.TABLEAU_COLORS[\"tab:orange\"],\n",
    "              mcolors.TABLEAU_COLORS[\"tab:purple\"],\n",
    "              mcolors.TABLEAU_COLORS[\"tab:cyan\"],\n",
    "              mcolors.TABLEAU_COLORS[\"tab:brown\"],\n",
    "              ]\n",
    "    \n",
    "    plt.bar(model, returns, color=colors)\n",
    "    plt.ylabel(\"Average Return\")\n",
    "    plt.xlabel(\"Agent\")\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "    plt.title(f\"Average return by Agent with {size} episodes\")\n",
    "    \n",
    "    handles = [plt.Rectangle((0,0), 1, 1, color=color) for color in colors]\n",
    "    plt.legend(handles, model, loc=\"best\", bbox_to_anchor = (1, 1))\n",
    "    plt.show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call the function to actually plot our 3 plots, 1 plot for each **size** of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(100)\n",
    "plot(250)\n",
    "plot(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
