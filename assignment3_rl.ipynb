{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 579 - Assignment 3\n",
    "[Ling Fei Zhang](https://github.com/Ling01234), 260985358\n",
    "\n",
    "Sevag Baghdassarian, ID\n",
    "\n",
    "Brandon Ma, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import time\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import sem\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions:\n",
    "# 0: left\n",
    "# 1: right\n",
    "\n",
    "# best params initialization:\n",
    "ALPHA = 1/4\n",
    "EPSILON = 0.25\n",
    "GAMMA = 0.95\n",
    "BINS = 10\n",
    "EPISODES = 1000\n",
    "RUNS = 10\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "class Qlearning:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, num_episodes, num_bins, seed) -> None:\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_bins = num_bins\n",
    "        self.lowerbound = env.observation_space.low\n",
    "        self.lowerbound[1] = -3.5\n",
    "        self.lowerbound[3] = -10\n",
    "        self.upperbound = env.observation_space.high\n",
    "        self.upperbound[1] = 3.5\n",
    "        self.upperbound[3] = 10\n",
    "        # self.env.seed(seed)\n",
    "        self.seed = seed\n",
    "        random.seed(self.seed)\n",
    "        self.num_action = env.action_space.n\n",
    "        self.reward = []\n",
    "        self.Qvalues = np.random.uniform(low=-0.001, high=0.001,\n",
    "                                         size=(num_bins, num_bins, num_bins, num_bins, self.num_action))\n",
    "        self.behavior_episodes1 = []\n",
    "        self.behavior_episodes2 = []\n",
    "        self.behavior_episodes3 = []\n",
    "        self.random_episodes1 = []\n",
    "        self.random_episodes2 = []\n",
    "        self.random_episodes3 = []\n",
    "        self.bins = []\n",
    "        for i in range(4):\n",
    "            self.bins.append(np.linspace(\n",
    "                self.lowerbound[i], self.upperbound[i], self.num_bins))\n",
    "            \n",
    "    # def get_behavior_episodes(self):\n",
    "    #     return np.apply_along_axis(np.argmax, 4, self.Qvalues) #shape (10, 10, 10, 10)\n",
    "        \n",
    "\n",
    "    def discritize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discritize continuous state into a discrete state\n",
    "\n",
    "        Args:\n",
    "            state (list of length 4): Current continuous state of agent\n",
    "\n",
    "        Returns:\n",
    "            state (4-tuple): Current discritized state of agent\n",
    "        \"\"\"\n",
    "        new_state = []\n",
    "        for i in range(4):\n",
    "            index = np.maximum(np.digitize(state[i], self.bins[i]) - 1, 0)\n",
    "            new_state.append(index)\n",
    "\n",
    "        return tuple(new_state)\n",
    "\n",
    "    def select_action(self, state, episode):\n",
    "        \"\"\"\n",
    "        Select action given a state\n",
    "\n",
    "        Args:\n",
    "            state (4-tuple): Current state of the agent, continuous\n",
    "            episode (int): Current episode of the run\n",
    "\n",
    "        Returns:\n",
    "            int: Action chosen by the agent\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # lower exploration rate as we run many episodes\n",
    "        if episode > 700:\n",
    "            self.epsilon *= 0.99\n",
    "\n",
    "        # epsilon greedy\n",
    "        number = np.random.random()\n",
    "        if number < self.epsilon:  # uniformly choose action\n",
    "            return np.random.choice(self.num_action)\n",
    "\n",
    "        # greedy selection\n",
    "        state = self.discritize_state(state)\n",
    "        best_actions = np.where(\n",
    "            self.Qvalues[state] == np.max(self.Qvalues[state]))[0]\n",
    "        return np.random.choice(best_actions)\n",
    "\n",
    "    def simulate_episodes(self):\n",
    "        \"\"\"\n",
    "        Simulate a specified number of episodes\n",
    "        \"\"\"\n",
    "        for episode in range(1, self.num_episodes+1):\n",
    "            # reset env\n",
    "            (state, _) = self.env.reset()\n",
    "            state = list(state)\n",
    "\n",
    "            # run episode\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                action = self.select_action(state, episode)\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                next_discritized_state = self.discritize_state(\n",
    "                    list(next_state))\n",
    "\n",
    "                q_max = np.max(self.Qvalues[next_discritized_state])\n",
    "                self.qlearning_update(\n",
    "                    terminal, reward, action, discritized_state, q_max)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            self.reward.append(int(episode_reward))\n",
    "\n",
    "    def qlearning_update(self, terminal, reward, action, state, q_max):\n",
    "        \"\"\"\n",
    "        Qlearning update rule\n",
    "\n",
    "        Args:\n",
    "            terminal (bool): True if at terminal state, False otherwise\n",
    "            reward (int): Reward of the agent at current state\n",
    "            action (int): Action taken by agent\n",
    "            state (4-tuple): Discrete state of the agent\n",
    "            q_max (float): Max Q value of the next state\n",
    "        \"\"\"\n",
    "        if not terminal:\n",
    "            loss = reward + self.gamma * q_max - \\\n",
    "                self.Qvalues[state + (action,)]\n",
    "        else:\n",
    "            loss = reward - self.Qvalues[state + (action,)]\n",
    "\n",
    "        self.Qvalues[state + (action,)] += self.alpha * loss\n",
    "\n",
    "    def visualize(self, games):\n",
    "        \"\"\"\n",
    "        Visualize the game played for a specified number of games.\n",
    "        Prints out the reward for each game.\n",
    "\n",
    "        Args:\n",
    "            games (int): Number of games to be played\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        for game in range(games):\n",
    "            (state, _) = env.reset()\n",
    "            env.render()\n",
    "            rewards = 0\n",
    "\n",
    "            for _ in range(500):\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                best_actions = np.where(self.Qvalues[discritized_state] == np.max(\n",
    "                    self.Qvalues[discritized_state]))[0]\n",
    "                action = np.random.choice(best_actions)\n",
    "                (state, reward, terminal, _, _) = env.step(action)\n",
    "                rewards += int(reward)\n",
    "                time.sleep(0.05)\n",
    "\n",
    "                if terminal:\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "            print(f\"reward for game {game}: {rewards}\")\n",
    "        env.close()\n",
    "        \n",
    "    def gather_episodes_agent(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Gather num_episodes behavior episodes for simple imitation learning\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): number of behavior episodes desired\n",
    "\n",
    "        Returns:\n",
    "            int: return of simple imitation learning using Q-Learning Agent\n",
    "            as expert.\n",
    "        \"\"\"\n",
    "        self.simulate_episodes()\n",
    "        print(f\"reward after simulate_episode: {self.reward[-20:]}\")\n",
    "        for episode in trange(1, num_episodes+1):\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"gather episode {episode}\")\n",
    "            state, _ = self.env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                best_actions = np.where(self.Qvalues[discritized_state] == np.max(\n",
    "                    self.Qvalues[discritized_state]))[0]\n",
    "                action = np.random.choice(best_actions)\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                \n",
    "                if episode <= 100:\n",
    "                    self.behavior_episodes1.append((state, action, reward, next_state, terminal))\n",
    "                    self.behavior_episodes2.append((state, action, reward, next_state, terminal))\n",
    "                if episode <= 250:\n",
    "                    self.behavior_episodes2.append((state, action, reward, next_state, terminal))\n",
    "                self.behavior_episodes3.append((state, action, reward, next_state, terminal))\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "        # preprocess data\n",
    "        x = np.array([data[0] for data in self.behavior_episodes3])\n",
    "        y = np.array([data[1] for data in self.behavior_episodes3])\n",
    "                \n",
    "        episode_reward = simple_imitation(x, y)\n",
    "        return episode_reward\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def gather_episodes_random(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Gather num_episodes behavior episodes with a random agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): number of behavior episodes desired for simple imitation\n",
    "\n",
    "        Returns:\n",
    "            int: return by a random agent. \n",
    "        \"\"\"\n",
    "        # print(f\"reward after simulate_episode: {self.reward[-20:]}\")\n",
    "        for episode in trange(1, num_episodes+1):\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"random agent episode {episode}\")\n",
    "            state, _ = self.env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                discritized_state = self.discritize_state(state)\n",
    "                action = self.env.action_space.sample()\n",
    "                (next_state, reward, terminal, _, _) = self.env.step(action)\n",
    "                \n",
    "                if episode <= 100:\n",
    "                    self.random_episodes1.append((state, action, reward, next_state, terminal))\n",
    "                    self.random_episodes2.append((state, action, reward, next_state, terminal))\n",
    "                if episode <= 250:\n",
    "                    self.random_episodes2.append((state, action, reward, next_state, terminal))\n",
    "                self.random_episodes3.append((state, action, reward, next_state, terminal))\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "        # preprocess data\n",
    "        x = np.array([data[0] for data in self.random_episodes3])\n",
    "        y = np.array([data[1] for data in self.random_episodes3])\n",
    "        \n",
    "        episode_reward = simple_imitation(x, y)\n",
    "        return episode_reward\n",
    "        \n",
    "\n",
    "def simple_imitation(x, y):\n",
    "    \"\"\"\n",
    "    Simple imitation estimator\n",
    "\n",
    "    Args:\n",
    "        x (list): list of states from dataset\n",
    "        y (list): list of actions from dataset\n",
    "\n",
    "    Returns:\n",
    "        int: return observed by the agent during testing\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        action = model.predict(state.reshape(1, -1))[0]\n",
    "        state, reward, terminal, *_ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    env.close()\n",
    "    return episode_reward\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will run a test run on the model, and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward after simulate_episode: [153, 122, 131, 148, 119, 167, 119, 163, 144, 140, 142, 151, 140, 149, 192, 143, 133, 175, 150, 111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 219.52it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1875.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple imitation with expert agent reward: 175.0\n",
      "Simple imitation with random agent reward: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    qlearning = Qlearning(env, ALPHA, GAMMA, EPSILON, EPISODES, BINS, SEED)\n",
    "    model_rewards = qlearning.gather_episodes_agent(500)\n",
    "    random_rewards = qlearning.gather_episodes_random(500)\n",
    "    print(f\"Simple imitation with expert agent reward: {model_rewards}\")\n",
    "    print(f\"Simple imitation with random agent reward: {random_rewards}\")\n",
    "    env.close()\n",
    "    return qlearning.behavior_episodes1, qlearning.random_episodes1, qlearning.behavior_episodes2, qlearning.random_episodes2, qlearning.behavior_episodes3, qlearning.random_episodes3\n",
    "    \n",
    "data1, data2, data3, data4, data5, data6 = test_model()    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-trained our Q-Learning agent with 1000 episodes, as we did in the previous assignment. We then use the trained Q-Learning agent as our expert in simple imitation learning and used logisitc regression to imitate the action observed in each state. The results above were produced using 500 behavior episodes. We can see that we can get decent results from simple imitation learning. On the other hand, we can see the returns received by the random agent. Without surprise, the returns are very low. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Below, we will create the 9 datasets we need to perform our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert Q-learning data\n",
    "# 100 episodes of expert data\n",
    "x1 = np.array([data[0] for data in data1])\n",
    "y1 = np.array([data[1] for data in data1])\n",
    "\n",
    "# 250 episodes of expert data\n",
    "x3 = np.array([data[0] for data in data3])\n",
    "y3 = np.array([data[1] for data in data3])\n",
    "\n",
    "# 500 episodes of expert data\n",
    "x5 = np.array([data[0] for data in data5])\n",
    "y5 = np.array([data[1] for data in data5])\n",
    "\n",
    "\n",
    "\n",
    "# random agent data\n",
    "# 100 episodes of random data\n",
    "x2 = np.array([data[0] for data in data2])\n",
    "y2 = np.array([data[1] for data in data2])\n",
    "\n",
    "# 250 episodes of random data\n",
    "x4 = np.array([data[0] for data in data4])\n",
    "y4 = np.array([data[1] for data in data4])\n",
    "\n",
    "# 500 episodes of random data\n",
    "x6 = np.array([data[0] for data in data6])\n",
    "y6 = np.array([data[1] for data in data6])\n",
    "\n",
    "# Shuffled data\n",
    "shuffled_data = data5 + data6\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "# 100 episodes of mixed data\n",
    "data7 = shuffled_data[:int(len(shuffled_data)/10)]\n",
    "x7 = np.array([data[0] for data in data7])\n",
    "y7 = np.array([data[1] for data in data7])\n",
    "\n",
    "# 250 episodes of mixed data\n",
    "data8 = shuffled_data[:int(len(shuffled_data)/4)]\n",
    "x8 = np.array([data[0] for data in data8])\n",
    "y8 = np.array([data[1] for data in data8])\n",
    "\n",
    "# 500 episodes of mixed data\n",
    "data9 = shuffled_data[:int(len(shuffled_data)/2)]\n",
    "x9 = np.array([data[0] for data in data9])\n",
    "y9 = np.array([data[1] for data in data9])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitted Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:00<00:03, 133.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qvalue in predict: [-0.00077002  0.00037977]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [-0.00050277  0.00047545]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [-0.00022999  0.00057355]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [4.98719434e-05 6.74171419e-04]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [0.00033835 0.00077742]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [0.00063695 0.00088341]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [0.00094711 0.00099223]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [0.00127018 0.00110398]\n",
      "action in select_action: 0\n",
      "qvalue in predict: [0.00105326 0.00102581]\n",
      "action in select_action: 0\n",
      "qvalue in predict: [0.00084225 0.00094601]\n",
      "action in select_action: 1\n",
      "qvalue in predict: [0.00118548 0.00105713]\n",
      "action in select_action: 0\n",
      "qvalue in predict: [-0.00065254  0.0006969 ]\n",
      "qvalue in predict: [-0.00064008  0.00063339]\n",
      "qvalue in predict: [-0.00116625  0.00023746]\n",
      "qvalue in predict: [-0.00047462  0.00062065]\n",
      "qvalue in predict: [-0.00054641  0.00052912]\n",
      "qvalue in predict: [2.24630481e-05 1.19962161e-03]\n",
      "qvalue in predict: [-0.00094767  0.00094031]\n",
      "qvalue in predict: [-0.00089226  0.00078581]\n",
      "qvalue in predict: [-3.07845184e-05  1.41057242e-03]\n",
      "qvalue in predict: [-0.00106129  0.0003243 ]\n",
      "qvalue in predict: [-0.0007844   0.00046199]\n",
      "qvalue in predict: [0.00063796 0.00142239]\n",
      "qvalue in predict: [4.06974668e-05 1.14341962e-03]\n",
      "qvalue in predict: [0.00020809 0.00117133]\n",
      "qvalue in predict: [-0.00041141  0.00068236]\n",
      "qvalue in predict: [-0.00071086  0.00049459]\n",
      "qvalue in predict: [-0.00080316  0.00054886]\n",
      "qvalue in predict: [-0.00056564  0.00062959]\n",
      "qvalue in predict: [-0.00055483  0.00053627]\n",
      "qvalue in predict: [-0.00024491  0.00063582]\n",
      "qvalue in predict: [-0.00079925  0.00043627]\n",
      "qvalue in predict: [-0.00050539  0.00062071]\n",
      "qvalue in predict: [-0.0008419   0.00038464]\n",
      "qvalue in predict: [-0.00083974  0.00051931]\n",
      "qvalue in predict: [-0.00018077  0.00086504]\n",
      "qvalue in predict: [7.69017638e-05 1.25415744e-03]\n",
      "qvalue in predict: [-0.00284549 -0.00034221]\n",
      "qvalue in predict: [-0.00049824  0.00065935]\n",
      "qvalue in predict: [-1.32208284e-05  6.65559693e-04]\n",
      "qvalue in predict: [-0.00086098  0.00057514]\n",
      "qvalue in predict: [0.00059644 0.00145687]\n",
      "qvalue in predict: [-0.00083297  0.00044666]\n",
      "qvalue in predict: [-1.83147020e-03  9.75813622e-06]\n",
      "qvalue in predict: [-0.00043129  0.00064537]\n",
      "qvalue in predict: [-0.00067237  0.00064512]\n",
      "qvalue in predict: [-0.00056409  0.00061416]\n",
      "qvalue in predict: [0.00079326 0.00148618]\n",
      "qvalue in predict: [-0.00095127  0.0002966 ]\n",
      "qvalue in predict: [0.00118491 0.00155473]\n",
      "qvalue in predict: [-0.00082615  0.00079121]\n",
      "qvalue in predict: [-0.00067812  0.0005405 ]\n",
      "qvalue in predict: [0.00074792 0.00169175]\n",
      "qvalue in predict: [-0.00076315  0.00061782]\n",
      "qvalue in predict: [0.00200578 0.00190789]\n",
      "qvalue in predict: [-0.00054587  0.00051208]\n",
      "qvalue in predict: [-0.00088642  0.00078051]\n",
      "qvalue in predict: [-0.00068088  0.00109005]\n",
      "qvalue in predict: [-0.00109979  0.0008274 ]\n",
      "qvalue in predict: [-0.00026786  0.00063274]\n",
      "qvalue in predict: [-0.00070575  0.00064861]\n",
      "qvalue in predict: [-0.00094767  0.00094031]\n",
      "qvalue in predict: [-0.0006739   0.00066748]\n",
      "qvalue in predict: [-0.00095657  0.00028262]\n",
      "qvalue in predict: [0.00136555 0.00166838]\n",
      "qvalue in predict: [-0.00034556  0.00087727]\n",
      "qvalue in predict: [-0.00071878  0.00046886]\n",
      "qvalue in predict: [-0.00051492  0.00053214]\n",
      "qvalue in predict: [-0.00132644  0.00015643]\n",
      "qvalue in predict: [-0.00052624  0.00064178]\n",
      "qvalue in predict: [0.0003713  0.00116748]\n",
      "qvalue in predict: [-0.00095012  0.00057467]\n",
      "qvalue in predict: [-0.00082195  0.00051657]\n",
      "qvalue in predict: [0.00045489 0.00148753]\n",
      "next_qvalues in update: [-0.0006525364191016234, -0.0006400833231328756, -0.0011662486868739422, -0.0004746207945714796, -0.000546412938794507, 2.24630480612373e-05, -0.0009476710441598549, -0.000892255230271286, -3.0784518406053716e-05, -0.0010612893744839699, -0.0007843962908113223, 0.0006379638277576796, 4.069746676640495e-05, 0.00020808811359038866, -0.0004114052770632124, 0, -0.000710863804065261, -0.0008031585370776825, -0.000565643866520155, -0.0005548318574714502, -0.0002449108176946729, -0.0007992545107875649, -0.0005053877718396965, -0.0008418975611187242, -0.0008397362286234552, -0.00018076957982564876, 7.690176376100679e-05, -0.0028454860211549605, -0.0004982373307615931, -1.3220828411493143e-05, -0.0008609828713496555, 0.0005964411079467177, -0.0008329703557675055, -0.0018314701975870895, -0.0004312944919638456, -0.0006723715322679874, -0.0005640942350014814, 0.0007932581173284941, -0.0009512652589608168, 0.0011849126705226311, -0.0008261456070443648, -0.0006781212486919901, 0.0007479158001676137, -0.0007631534514545642, 0.0020057758219116867, -0.0005458721934674616, -0.0008864173538213739, -0.0006808818428537632, -0.0010997883310800316, -0.00026786491765773884, -0.000705750659831645, -0.0009476710441598549, -0.0006738986222867891, -0.0009565671579207681, 0.0013655457717963522, -0.00034556043587503807, -0.0007187755092580192, -0.0005149211435893648, -0.001326443751825732, -0.0005262415602894873, 0.00037130032166804515, -0.0009501221796860205, -0.0008219532195182658, 0.00045489414880858466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 4 features, but LinearRegression is expecting 5 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ling0\\Dropbox\\School\\McGill\\Winter 2023\\COMP 579\\A3\\assignment3_rl.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting reward: \u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m test_model_fitted()\n",
      "\u001b[1;32mc:\\Users\\Ling0\\Dropbox\\School\\McGill\\Winter 2023\\COMP 579\\A3\\assignment3_rl.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m model \u001b[39m=\u001b[39m FittedQLearning(env, behavior_episode)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m reward \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting reward: \u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Ling0\\Dropbox\\School\\McGill\\Winter 2023\\COMP 579\\A3\\assignment3_rl.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_action(state)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     next_state, reward, done, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "\u001b[1;32mc:\\Users\\Ling0\\Dropbox\\School\\McGill\\Winter 2023\\COMP 579\\A3\\assignment3_rl.ipynb Cell 11\u001b[0m in \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_action\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     qvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(qvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maction in select_action: \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Ling0\\Dropbox\\School\\McGill\\Winter 2023\\COMP 579\\A3\\assignment3_rl.ipynb Cell 11\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# qvalue = self.model.predict(np.array(state).reshape(1, -1))[0]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     qvalue \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mqvalue in predict: \u001b[39m\u001b[39m{\u001b[39;00mqvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ling0/Dropbox/School/McGill/Winter%202023/COMP%20579/A3/assignment3_rl.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m qvalue\n",
      "File \u001b[1;32mc:\\Users\\Ling0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\Ling0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\Users\\Ling0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Ling0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 4 features, but LinearRegression is expecting 5 features as input."
     ]
    }
   ],
   "source": [
    "class FittedQLearning:\n",
    "    def __init__(self, env, buffer, gamma=0.99, num_episodes=500, batch_size=64, buffer_size=5000, approximator=\"linear\"):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = buffer\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.approximator = approximator\n",
    "\n",
    "        # build model\n",
    "        if approximator == \"linear\":\n",
    "            self.model = LinearRegression()\n",
    "        elif approximator == \"mlp\":\n",
    "            self.model = MLPRegressor(hidden_layer_sizes=(\n",
    "                64,), activation=\"relu\", solver=\"adam\", max_iter=1000)\n",
    "        else:\n",
    "            raise Exception(\"The approximator must be 'linear' or 'mlp'\")\n",
    "\n",
    "        # initial fit of the approximator\n",
    "        self.initial_fit()\n",
    "\n",
    "        # initial buffer\n",
    "        for data in self.buffer[:self.buffer_size]:\n",
    "            self.remember(*data)\n",
    "\n",
    "    def initial_fit(self):\n",
    "        if self.approximator == \"linear\":\n",
    "            weights = np.random.uniform(low=-0.001, high=0.001,\n",
    "                                        size=(2, self.state_space))\n",
    "            weights_inter = np.random.uniform(low=-0.001, high=0.001,\n",
    "                                        size=(2,))\n",
    "            \n",
    "            self.model.coef_ = weights\n",
    "            self.model.intercept_ = weights_inter\n",
    "\n",
    "        else: # TO MODIFY WITH PROPER SIZE\n",
    "            layer_sizes = self.model.hidden_layer_sizes + (1,)\n",
    "            weights = []\n",
    "            for i in range(len(layer_sizes) - 1):\n",
    "                w = [np.random.uniform(low=-0.001, high=0.001,\n",
    "                                       size=(layer_sizes[i], layer_sizes[i+1]))]\n",
    "                weights.append(w)\n",
    "            self.model.coefs_ = weights\n",
    "        # self.model.fit(self.buffer[:, :4], self.buffer[:, 4])\n",
    "\n",
    "    def predict(self, state):\n",
    "        # qvalue = self.model.predict(np.array(state).reshape(1, -1))[0]\n",
    "        qvalue = self.model.predict(state.reshape(1,-1))[0]\n",
    "        print(f\"qvalue in predict: {qvalue}\")\n",
    "        return qvalue\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) > self.batch_size:\n",
    "            batch_index = np.random.choice(\n",
    "                len(self.buffer), size=self.batch_size)\n",
    "            batch = [self.buffer[i] for i in batch_index]\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = np.array(states)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            next_states = np.array(next_states)\n",
    "            dones = np.array([int(done) for done in dones])\n",
    "\n",
    "            # axis 1 checks for each next_state\n",
    "            next_qvalues = []\n",
    "            for index, next_state in enumerate(next_states):\n",
    "                if dones[index] == 1:\n",
    "                    qvalues = 0\n",
    "                else:\n",
    "                    qvalues = np.max(self.predict(next_state.reshape(1, -1))[0])\n",
    "                next_qvalues.append(qvalues)\n",
    "            print(f\"next_qvalues in update: {next_qvalues}\")\n",
    "            target_qvalues = rewards + self.gamma * (1 - dones) * next_qvalues\n",
    "\n",
    "            if self.approximator == \"linear\":\n",
    "                x = np.hstack([states, actions.reshape(-1, 1)])\n",
    "                # x = states\n",
    "            else:\n",
    "                x = states\n",
    "            y = target_qvalues.reshape(-1, 1)\n",
    "\n",
    "            self.model.fit(x, y)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        qvalues = self.predict(state)\n",
    "        action = np.argmax(qvalues)\n",
    "        print(f\"action in select_action: {action}\")\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        for episode in trange(1, self.num_episodes+1):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, *_ = self.env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "\n",
    "            self.update()\n",
    "\n",
    "            # if episode % 10 == 0:\n",
    "            #     print(f\"episode {episode} in training\")\n",
    "\n",
    "    def test(self):\n",
    "        state, _ = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            state, reward, done, *_ = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        return episode_reward\n",
    "                    \n",
    "def test_model_fitted():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    model = FittedQLearning(env, behavior_episode)\n",
    "    model.train()\n",
    "    reward = model.test()\n",
    "    print(f\"testing reward: {reward}\")\n",
    "    env.close()\n",
    "    \n",
    "test_model_fitted()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
